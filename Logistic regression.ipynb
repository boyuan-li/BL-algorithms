{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List, TypeVar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x: float) -> float:\n",
    "    return 1.0 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_prime(x: float) -> float:\n",
    "    y = logistic(x)\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vector = List[float]\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    '''Computes dot product'''\n",
    "    assert len(v) == len(w), 'vectors must be same length'\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_likelihood(x: Vector, y: float, beta: Vector) -> float:\n",
    "    '''The negative log likelihood for one data point'''\n",
    "    if y == 1:\n",
    "        return -math.log(logistic(dot(x, beta)))\n",
    "    else:\n",
    "        return -math.log(1 - logistic(dot(x, beta)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_log_likelihood(xs: List[Vector],\n",
    "                            ys: List[float],\n",
    "                            beta: Vector) -> float:\n",
    "    return sum(_negative_log_likelihood(x, y, beta)\n",
    "               for x, y in zip(xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    '''Sums all corresponding elements'''\n",
    "    # check that vectors is not empty \n",
    "    assert vectors, 'no vectors provided'\n",
    "    \n",
    "    # check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), 'different sizes'\n",
    "    \n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "assert vector_sum([[1,2],[3,4],[5,6],[7,8]]) == [16,20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _negative_log_partial_j(x: Vector, y: float, beta: Vector, j: int) -> float:\n",
    "    '''The jtj partial derivative for one data point. i is the index of the data point'''\n",
    "    return -(y - logistic(dot(x, beta))) * x[j]\n",
    "\n",
    "def _negative_log_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    '''The gradient for one data point'''\n",
    "    return [_negative_log_partial_j(x, y, beta, j)\n",
    "            for j in range(len(beta))]\n",
    "\n",
    "def negative_log_gradient(xs: List[Vector],\n",
    "                          ys: List[float],\n",
    "                          beta: Vector) -> Vector:\n",
    "    return vector_sum([_negative_log_gradient(x, y, beta)\n",
    "                       for x, y in zip(xs, ys)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "least squares fit: 100%|██████████| 1000/1000 [00:01<00:00, 925.93it/s]\n"
     ]
    }
   ],
   "source": [
    "tuples = [(0.7,48000,1),(1.9,48000,0),(2.5,60000,1),(4.2,63000,0),(6,76000,0),(6.5,69000,0),(7.5,76000,0),(8.1,88000,0),(8.7,83000,1),(10,83000,1),(0.8,43000,0),(1.8,60000,0),(10,79000,1),(6.1,76000,0),(1.4,50000,0),(9.1,92000,0),(5.8,75000,0),(5.2,69000,0),(1,56000,0),(6,67000,0),(4.9,74000,0),(6.4,63000,1),(6.2,82000,0),(3.3,58000,0),(9.3,90000,1),(5.5,57000,1),(9.1,102000,0),(2.4,54000,0),(8.2,65000,1),(5.3,82000,0),(9.8,107000,0),(1.8,64000,0),(0.6,46000,1),(0.8,48000,0),(8.6,84000,1),(0.6,45000,0),(0.5,30000,1),(7.3,89000,0),(2.5,48000,1),(5.6,76000,0),(7.4,77000,0),(2.7,56000,0),(0.7,48000,0),(1.2,42000,0),(0.2,32000,1),(4.7,56000,1),(2.8,44000,1),(7.6,78000,0),(1.1,63000,0),(8,79000,1),(2.7,56000,0),(6,52000,1),(4.6,56000,0),(2.5,51000,0),(5.7,71000,0),(2.9,65000,0),(1.1,33000,1),(3,62000,0),(4,71000,0),(2.4,61000,0),(7.5,75000,0),(9.7,81000,1),(3.2,62000,0),(7.9,88000,0),(4.7,44000,1),(2.5,55000,0),(1.6,41000,0),(6.7,64000,1),(6.9,66000,1),(7.9,78000,1),(8.1,102000,0),(5.3,48000,1),(8.5,66000,1),(0.2,56000,0),(6,69000,0),(7.5,77000,0),(8,86000,0),(4.4,68000,0),(4.9,75000,0),(1.5,60000,0),(2.2,50000,0),(3.4,49000,1),(4.2,70000,0),(7.7,98000,0),(8.2,85000,0),(5.4,88000,0),(0.1,46000,0),(1.5,37000,0),(6.3,86000,0),(3.7,57000,0),(8.4,85000,0),(2,42000,0),(5.8,69000,1),(2.7,64000,0),(3.1,63000,0),(1.9,48000,0),(10,72000,1),(0.2,45000,0),(8.6,95000,0),(1.5,64000,0),(9.8,95000,0),(5.3,65000,0),(7.5,80000,0),(9.9,91000,0),(9.7,50000,1),(2.8,68000,0),(3.6,58000,0),(3.9,74000,0),(4.4,76000,0),(2.5,49000,0),(7.2,81000,0),(5.2,60000,1),(2.4,62000,0),(8.9,94000,0),(2.4,63000,0),(6.8,69000,1),(6.5,77000,0),(7,86000,0),(9.4,94000,0),(7.8,72000,1),(0.2,53000,0),(10,97000,0),(5.5,65000,0),(7.7,71000,1),(8.1,66000,1),(9.8,91000,0),(8,84000,0),(2.7,55000,0),(2.8,62000,0),(9.4,79000,0),(2.5,57000,0),(7.4,70000,1),(2.1,47000,0),(5.3,62000,1),(6.3,79000,0),(6.8,58000,1),(5.7,80000,0),(2.2,61000,0),(4.8,62000,0),(3.7,64000,0),(4.1,85000,0),(2.3,51000,0),(3.5,58000,0),(0.9,43000,0),(0.9,54000,0),(4.5,74000,0),(6.5,55000,1),(4.1,41000,1),(7.1,73000,0),(1.1,66000,0),(9.1,81000,1),(8,69000,1),(7.3,72000,1),(3.3,50000,0),(3.9,58000,0),(2.6,49000,0),(1.6,78000,0),(0.7,56000,0),(2.1,36000,1),(7.5,90000,0),(4.8,59000,1),(8.9,95000,0),(6.2,72000,0),(6.3,63000,0),(9.1,100000,0),(7.3,61000,1),(5.6,74000,0),(0.5,66000,0),(1.1,59000,0),(5.1,61000,0),(6.2,70000,0),(6.6,56000,1),(6.3,76000,0),(6.5,78000,0),(5.1,59000,0),(9.5,74000,1),(4.5,64000,0),(2,54000,0),(1,52000,0),(4,69000,0),(6.5,76000,0),(3,60000,0),(4.5,63000,0),(7.8,70000,0),(3.9,60000,1),(0.8,51000,0),(4.2,78000,0),(1.1,54000,0),(6.2,60000,0),(2.9,59000,0),(2.1,52000,0),(8.2,87000,0),(4.8,73000,0),(2.2,42000,1),(9.1,98000,0),(6.5,84000,0),(6.9,73000,0),(5.1,72000,0),(9.1,69000,1),(9.8,79000,1),]\n",
    "data = [list(row) for row in tuples]\n",
    "\n",
    "xs = [[1.0] + row[:2] for row in data]  # [1, experience, salary]\n",
    "ys = [row[2] for row in data]  \n",
    "\n",
    "def add(v: Vector, w: Vector) -> Vector:\n",
    "    \"\"\"Adds corresponding elements\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be the same length\"\n",
    "\n",
    "    return [v_i + w_i for v_i, w_i in zip(v, w)]\n",
    "\n",
    "def mean(xs: List[float]) -> float:\n",
    "    return sum(xs) / len(xs)\n",
    "\n",
    "def de_mean(xs: List[float]) -> List[float]:\n",
    "    \"\"\"Translate xs by subtracting its mean (so the result has mean 0)\"\"\"\n",
    "    x_bar = mean(xs)\n",
    "    return [x - x_bar for x in xs]\n",
    "\n",
    "def dot(v: Vector, w: Vector) -> float:\n",
    "    \"\"\"Computes v_1 * w_1 + ... + v_n * w_n\"\"\"\n",
    "    assert len(v) == len(w), \"vectors must be same length\"\n",
    "\n",
    "    return sum(v_i * w_i for v_i, w_i in zip(v, w))\n",
    "\n",
    "def variance(xs: List[float]) -> float:\n",
    "    \"\"\"Almost the average squared deviation from the mean\"\"\"\n",
    "    assert len(xs) >= 2, \"variance requires at least two elements\"\n",
    "\n",
    "    n = len(xs)\n",
    "    deviations = de_mean(xs)\n",
    "    return sum_of_squares(deviations) / (n - 1)\n",
    "\n",
    "import math\n",
    "\n",
    "def standard_deviation(xs: List[float]) -> float:\n",
    "    \"\"\"The standard deviation is the square root of the variance\"\"\"\n",
    "    return math.sqrt(variance(xs))\n",
    "\n",
    "def sum_of_squares(v: Vector) -> float:\n",
    "    \"\"\"Returns v_1 * v_1 + ... + v_n * v_n\"\"\"\n",
    "    return dot(v, v)\n",
    "\n",
    "def vector_sum(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Sums all corresponding elements\"\"\"\n",
    "    # Check that vectors is not empty\n",
    "    assert vectors, \"no vectors provided!\"\n",
    "\n",
    "    # Check the vectors are all the same size\n",
    "    num_elements = len(vectors[0])\n",
    "    assert all(len(v) == num_elements for v in vectors), \"different sizes!\"\n",
    "\n",
    "    # the i-th element of the result is the sum of every vector[i]\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def scalar_multiply(c: float, v: Vector) -> Vector:\n",
    "    \"\"\"Multiplies every element by c\"\"\"\n",
    "    return [c * v_i for v_i in v]\n",
    "\n",
    "def vector_mean(vectors: List[Vector]) -> Vector:\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "\n",
    "def scale(data: List[Vector]) -> Tuple[Vector, Vector]:\n",
    "    \"\"\"returns the means and standard deviations for each position\"\"\"\n",
    "    dim = len(data[0])\n",
    "\n",
    "    means = vector_mean(data)\n",
    "    stdevs = [standard_deviation([vector[i] for vector in data])\n",
    "              for i in range(dim)]\n",
    "\n",
    "    return means, stdevs\n",
    "\n",
    "def rescale(data: List[Vector]) -> List[Vector]:\n",
    "    \"\"\"\n",
    "    Rescales the input data so that each position has\n",
    "    mean 0 and standard deviation 1. (Leaves a position\n",
    "    as is if its standard deviation is 0.)\n",
    "    \"\"\"\n",
    "    dim = len(data[0])\n",
    "    means, stdevs = scale(data)\n",
    "\n",
    "    # Make a copy of each vector\n",
    "    rescaled = [v[:] for v in data]\n",
    "\n",
    "    for v in rescaled:\n",
    "        for i in range(dim):\n",
    "            if stdevs[i] > 0:\n",
    "                v[i] = (v[i] - means[i]) / stdevs[i]\n",
    "\n",
    "    return rescaled\n",
    "\n",
    "def least_squares_fit(xs: List[Vector],\n",
    "                      ys: List[float],\n",
    "                      learning_rate: float = 0.001,\n",
    "                      num_steps: int = 1000,\n",
    "                      batch_size: int = 1) -> Vector:\n",
    "    \"\"\"\n",
    "    Find the beta that minimizes the sum of squared errors\n",
    "    assuming the model y = dot(x, beta).\n",
    "    \"\"\"\n",
    "    # Start with a random guess\n",
    "    guess = [random.random() for _ in xs[0]]\n",
    "\n",
    "    for _ in tqdm.trange(num_steps, desc=\"least squares fit\"):\n",
    "        for start in range(0, len(xs), batch_size):\n",
    "            batch_xs = xs[start:start+batch_size]\n",
    "            batch_ys = ys[start:start+batch_size]\n",
    "\n",
    "            gradient = vector_mean([sqerror_gradient(x, y, guess)\n",
    "                                    for x, y in zip(batch_xs, batch_ys)])\n",
    "            guess = gradient_step(guess, gradient, -learning_rate)\n",
    "\n",
    "    return guess\n",
    "\n",
    "def predict(x: Vector, beta: Vector) -> float:\n",
    "    \"\"\"assumes that the first element of x is 1\"\"\"\n",
    "    return dot(x, beta)\n",
    "\n",
    "def gradient_step(v: Vector, gradient: Vector, step_size: float) -> Vector:\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return predict(x, beta) - y\n",
    "\n",
    "def squared_error(x: Vector, y: float, beta: Vector) -> float:\n",
    "    return error(x, y, beta) ** 2\n",
    "\n",
    "x = [1, 2, 3]\n",
    "y = 30\n",
    "beta = [4, 4, 4]  # so prediction = 4 + 8 + 12 = 24\n",
    "\n",
    "assert error(x, y, beta) == -6\n",
    "assert squared_error(x, y, beta) == 36\n",
    "\n",
    "def sqerror_gradient(x: Vector, y: float, beta: Vector) -> Vector:\n",
    "    err = error(x, y, beta)\n",
    "    return [2 * err * x_i for x_i in x]\n",
    "\n",
    "learning_rate = 0.001\n",
    "rescaled_xs = rescale(xs)\n",
    "beta = least_squares_fit(rescaled_xs, ys, learning_rate, 1000, 1)\n",
    "\n",
    "predictions = [predict(x_i, beta) for x_i in rescaled_xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 39.963 beta: [-2.0239032476251424, 4.693047853942649, -4.469811321910748]: 100%|██████████| 5000/5000 [00:07<00:00, 653.19it/s]  \n"
     ]
    }
   ],
   "source": [
    "X = TypeVar('X') \n",
    "Y = TypeVar('Y') \n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Split data into fractions [prob, 1 - prob]\"\"\"\n",
    "    data = data[:]                    # Make a shallow copy\n",
    "    random.shuffle(data)              # because shuffle modifies the list.\n",
    "    cut = int(len(data) * prob)       # Use prob to find a cutoff\n",
    "    return data[:cut], data[cut:]     # and split the shuffled list there.\n",
    "\n",
    "def train_test_split(xs: List[X],\n",
    "                     ys: List[Y],\n",
    "                     test_pct: float) -> Tuple[List[X], List[X], List[Y], List[Y]]:\n",
    "    # Generate the indices and split them.\n",
    "    idxs = [i for i in range(len(xs))]\n",
    "    train_idxs, test_idxs = split_data(idxs, 1 - test_pct)\n",
    "\n",
    "    return ([xs[i] for i in train_idxs],  # x_train\n",
    "            [xs[i] for i in test_idxs],   # x_test\n",
    "            [ys[i] for i in train_idxs],  # y_train\n",
    "            [ys[i] for i in test_idxs])   # y_test\n",
    "\n",
    "random.seed(0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(rescaled_xs, ys, 0.33)\n",
    "learning_rate = 0.01\n",
    "\n",
    "beta = [random.random() for _ in range(3)]\n",
    "\n",
    "with tqdm.trange(5000) as t:\n",
    "    for epoch in t:\n",
    "        gradient = negative_log_gradient(x_train, y_train, beta)\n",
    "        beta = gradient_step(beta, gradient, -learning_rate)\n",
    "        loss = negative_log_likelihood(x_train, y_train, beta)\n",
    "        t.set_description(f\"loss: {loss:.3f} beta: {beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.0239032476251424, 4.693047853942649, -4.469811321910748]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.927236932527311, 1.6482026277676038, -0.00028768900920142336]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means, stdevs = scale(xs)\n",
    "beta_unscaled = [(beta[0] - beta[1] * means[1] / stdevs[1] - beta[2] * means[2] / stdevs[2]),\n",
    "                 beta[1] / stdevs[1],\n",
    "                 beta[2] / stdevs[2]]\n",
    "beta_unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.8\n"
     ]
    }
   ],
   "source": [
    "true_positives = false_positives = true_negatives = false_negatives = 0\n",
    "\n",
    "for x_i, y_i in zip(x_test, y_test):\n",
    "    prediction = logistic(dot(beta, x_i))\n",
    "    \n",
    "    if y_i == 1 and prediction >= 0.5: # TP: paid and we predicted paid \n",
    "        true_positives += 1\n",
    "    elif y_i == 1: # FN: paid and we predicted unpaid \n",
    "        false_negatives += 1\n",
    "    elif prediction >= 0.5: # FP: unpaid and we predict paid \n",
    "        false_positives += 1\n",
    "    else: # TN: unpaid and we predict unpaid \n",
    "        true_negatives += 1\n",
    "\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "print(precision, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = [logistic(dot(beta, x_i)) for x_i in x_test]\n",
    "plt.scatter(predictions, y_test, marker = '+')\n",
    "plt.xlabel('predicted probability')\n",
    "plt.ylabel('actual outcome')\n",
    "plt.title('Logistic Regression Predicted vs. Actual')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
